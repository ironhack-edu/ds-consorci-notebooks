{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An谩lisis de Regresi贸n (II)\n",
    "\n",
    "En este cuaderno, veremos c贸mo el an谩lisis de regresi贸n puede ayudar a **entender el comportamiento de los datos**, **predecir valores de datos** (continuos o dicot贸micos), y **encontrar predictores importantes** (modelos dispersos).\n",
    "Presentamos diferentes modelos de regresi贸n: regresi贸n lineal simple, regresi贸n lineal m煤ltiple y regresi贸n polin贸mica.\n",
    "Evaluamos los resultados cualitativamente mediante herramientas de visualizaci贸n de Seaborn y cuantitativamente mediante la biblioteca Scikit-learn, as铆 como otras herramientas.\n",
    "\n",
    "Usamos diferentes conjuntos de datos reales:\n",
    "* Conjunto de datos macroecon贸micos\n",
    "* Predicci贸n del precio de un nuevo mercado de viviendas\n",
    "* Extensi贸n del hielo marino y cambio clim谩tico\n",
    "* Conjunto de datos de diabetes de Scikit-learn\n",
    "* Conjunto de datos Longley de datos macroecon贸micos de EE. UU.\n",
    "* Conjunto de datos de publicidad\n",
    "\n",
    "### Contenidos del cuaderno:\n",
    "\n",
    "- Regresi贸n Lineal M煤ltiple\n",
    "- Regularizaci贸n: Ridge y Lasso\n",
    "- Transformaci贸n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for the visualizations\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline \n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1 # to make this notebook's output stable across runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 1: Vivienda en Boston\n",
    "\n",
    "Continuemos con nuestro conjunto de datos de Vivienda en Boston."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston() # Dictionary-like object that exposes its keys as attributes.\n",
    "X,y = boston.data, boston.target # Create X matrix and y vector from the dataset.\n",
    "features = boston.feature_names\n",
    "print('feature names: {}'.format(boston.feature_names))\n",
    "print('Shape of data: {} {}'.format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=seed)\n",
    "\n",
    "print('Train and test sizes of X: {} {}'.format(X_train.shape, X_test.shape))\n",
    "print('Train and test sizes of y: {} {}'.format(y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a multiple linear model\n",
    "lr = LinearRegression() # Create the Linear Regression estimator\n",
    "lr.fit(X_train, y_train) # Perform the fitting\n",
    "\n",
    "\n",
    "# Regrerssion coefs\n",
    "coefs_lr = pd.Series(np.abs(lr.coef_), features).sort_values()\n",
    "\n",
    "# Prediction\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "r2score_train = lr.score(X_train, y_train)\n",
    "r2score_test = lr.score(X_test, y_test)\n",
    "\n",
    "# The coefficients\n",
    "print('\\nIntercept and coefs:\\n{} {}'.format(lr.intercept_, lr.coef_))\n",
    "# The mean squared error\n",
    "print('\\nMSE: {}'.format(mse))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('R^2 Score: {}'.format(r2score_train))\n",
    "print('R^2 Score: {}'.format(r2score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting abs value of model coefficients\n",
    "coefs_lr.plot(kind='bar', title='Model Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que todos los coeficientes obtenidos son diferentes de cero, lo que significa que no se descarta ninguna variable.\n",
    "A continuaci贸n, intentaremos construir un nuevo modelo para predecir el precio utilizando los factores m谩s importantes y descartando los no informativos. Para hacer esto, podemos crear un regresor LASSO, forzando coeficientes a cero (ver m谩s abajo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Regularizaci贸n\n",
    "\n",
    "### Regularizaci贸n L2: Regresi贸n Ridge\n",
    "\n",
    "La Regresi贸n Ridge penaliza los coeficientes si estos est谩n demasiado alejados de cero, oblig谩ndolos a ser peque帽os de manera continua. De esta manera, reduce la complejidad del modelo mientras mantiene todas las variables en el modelo.\n",
    "\n",
    "$$ minimize(\\sum_{i=0}^n (y_i - \\beta_0- \\sum_{j=1}^p \\beta_jx_{ij})^2 + \\alpha\\sum_{j=1}^p \\beta_j^2) $$\n",
    "\n",
    "donde $\\beta_j$ son los coeficientes de regresi贸n.\n",
    "\n",
    "\n",
    "### Regularizaci贸n L1: Regresi贸n Lasso\n",
    "\n",
    "A menudo, en problemas reales, hay variables no informativas en los datos que impiden un modelado adecuado del problema y, por lo tanto, la construcci贸n de un modelo de regresi贸n correcto. En tales casos, un proceso de selecci贸n de caracter铆sticas es crucial para seleccionar solo las caracter铆sticas informativas y descartar las no informativas. Esto se puede lograr mediante m茅todos dispersos que utilizan un enfoque de penalizaci贸n, como *Lasso* (operador de encogimiento y selecci贸n absoluta m铆nima) para establecer algunos coeficientes del modelo a cero (descartando as铆 esas variables). La dispersi贸n puede verse como una aplicaci贸n de la navaja de Occam: preferir modelos m谩s simples a los complejos.\n",
    "\n",
    "Para ello, la regresi贸n Lasso a帽ade un t茅rmino de regularizaci贸n de **norma $\\ell_1$** a la suma de errores cuadr谩ticos de predicci贸n (SSE). Dado el conjunto de muestras (,), el objetivo es minimizar:\n",
    "\n",
    "$$ minimize(\\sum_{i=0}^n (y_i - \\beta_0- \\sum_{j=1}^p \\beta_jx_{ij})^2 + \\alpha\\sum_{j=1}^p|\\beta_j|)$$\n",
    "\n",
    "### Interpretaci贸n geom茅trica de la regularizaci贸n\n",
    "\n",
    "El panel izquierdo muestra la regularizaci贸n L1 (regularizaci贸n lasso) y el panel derecho la regularizaci贸n L2 (regresi贸n Ridge). Las elipses indican la distribuci贸n para no regularizaci贸n. Las formas (cuadrado y c铆rculo) muestran las restricciones debido a la regularizaci贸n (limitando $\\theta^2$ para la regresi贸n Ridge y $|\\theta|$ para la regresi贸n Lasso). Las esquinas de la regularizaci贸n L1 crean m谩s oportunidades para que la soluci贸n tenga ceros en algunos de los pesos.\n",
    "\n",
    "<center><img src=\"files/images/regularization-ridge-lasso.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M谩s informaci贸n [aqu铆](https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge Regression\n",
    "ridge = linear_model.Ridge(alpha=1) # Create a Ridge regressor\n",
    "ridge.fit(X_train, y_train) # Perform the fitting\n",
    "\n",
    "# Regrerssion coefs\n",
    "coefs_ridge = pd.Series(np.abs(ridge.coef_), features).sort_values()\n",
    "\n",
    "# Prediction\n",
    "y_test_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "mse_ridge = mean_squared_error(y_test, y_test_pred_ridge)\n",
    "r2score_ridge_train = ridge.score(X_train, y_train)\n",
    "r2score_ridge_test = ridge.score(X_test, y_test)\n",
    "\n",
    "# The coefficients\n",
    "print('\\nIntercept and coefs:\\n{} {}'.format(ridge.intercept_, ridge.coef_))\n",
    "# The mean squared error\n",
    "print('\\nMSE: {}'.format(mse_ridge))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('R^2 Score train: {}'.format(r2score_ridge_train))\n",
    "print('R^2 Score test: {}'.format(r2score_ridge_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting abs value of model coefficients\n",
    "coefs_ridge.plot(kind='bar', title='Ridge Coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para completar:**\n",
    "Ajusta un regresor Lasso y eval煤alo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lasso Regression\n",
    "lasso = linear_model.Lasso(alpha=1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Regrerssion coefs\n",
    "coefs_lasso = pd.Series(np.abs(lasso.coef_), features).sort_values()\n",
    "\n",
    "# Prediction\n",
    "y_test_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "mse_lasso = mean_squared_error(y_test, y_test_pred_lasso)\n",
    "\n",
    "r2score_lasso_train = lasso.score(X_train, y_train)\n",
    "r2score_lasso_test = lasso.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# The coefficients\n",
    "\n",
    "# The mean squared error\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(mse_lasso)\n",
    "print(r2score_lasso_train, r2score_lasso_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting abs value of model coefficients\n",
    "coefs_lasso.plot(kind='bar', title='Lasso Coefficients')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparar los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the coeficients now sparse?\n",
    "# Is the score different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15,5))\n",
    "ax1 = f.add_subplot(131)\n",
    "ax2 = f.add_subplot(132)\n",
    "ax3 = f.add_subplot(133)\n",
    "\n",
    "coefs_lr.plot(kind=\"barh\", title='coefs_lr', ax=ax1)\n",
    "coefs_ridge.plot(kind=\"barh\", title='coefs_ridge', ax=ax2)\n",
    "coefs_lasso.plot(kind=\"barh\", title='coefs_lasso', ax=ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Non important variables: {}'.format(coefs_lasso.index[coefs_lasso==0].values))\n",
    "print('Most important variable: {}'.format(coefs_lasso.index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[r2score_train, r2score_test],\n",
    "         [r2score_ridge_train, r2score_ridge_test],\n",
    "         [r2score_lasso_train, r2score_lasso_test]]\n",
    "df_scores = pd.DataFrame(scores, columns=[\"Train\", \"Test\"], index=[\"No regularization\", \"Ridge\", \"Lasso\"])\n",
    "#df_scores.sort_values(by=\"test_score\", ascending=False, inplace=True)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforma y Predice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at mean and average values of our predictors\n",
    "for i, feat in enumerate(features):\n",
    "    print()\n",
    "    print(feat)\n",
    "    print(\"Max {}, min {}, mean {}, and var {}\".format(np.max(X[:, i]), np.min(X[:, i]), np.mean(X[:, i]), np.var(X[:, i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe un tipo especial de ``Estimator`` llamado ``Transformer`` que transforma los datos de entrada, por ejemplo, selecciona un subconjunto de las caracter铆sticas o extrae nuevas caracter铆sticas basadas en las originales.\n",
    "\n",
    "Un transformador que utilizaremos aqu铆 es ``sklearn.preprocessing.StandardScaler``. Este transformador centra cada predictor en ``X`` para tener media cero y varianza unitaria y es 煤til."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalerX = StandardScaler().fit(X_train) # Create the transformer StandardScaler and perform the fitting for the training data\n",
    "\n",
    "X_train_norm = scalerX.transform(X_train)\n",
    "X_test_norm = scalerX.transform(X_test)\n",
    "\n",
    "print(\"\\nBefore transformation:\")\n",
    "print('Train: Max {}, min {}, mean {}, and var {}'.format(np.max(X_train), np.min(X_train), np.mean(X_train), np.var(X_train)))\n",
    "print('Test: Max {}, min {}, mean {}, and var {}'.format(np.max(X_test), np.min(X_test), np.mean(X_test), np.var(X_test)))\n",
    "\n",
    "print(\"\\nAfter transformation:\")\n",
    "print('Train: Max {}, min {}, mean {}, and var {}'.format(np.max(X_train_norm), np.min(X_train_norm), np.mean(X_train_norm), np.var(X_train_norm)))\n",
    "print('Test: Max {}, min {}, mean {}, and var {}'.format(np.max(X_test_norm), np.min(X_test_norm), np.mean(X_test_norm), np.var(X_test_norm)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feat in enumerate(features):\n",
    "    print()\n",
    "    print(feat)\n",
    "    print(\"\\nBefore transformation:\")\n",
    "    print('Train: Max {}, min {}, mean {}, and var {}'.format(np.max(X_train[:, i]), np.min(X_train[:, i]), np.mean(X_train[:, i]), np.var(X_train[:, i])))\n",
    "    print('Test: Max {}, min {}, mean {}, and var {}'.format(np.max(X_test[:, i]), np.min(X_test[:, i]), np.mean(X_test[:, i]), np.var(X_test[:, i])))\n",
    "\n",
    "    print(\"\\nAfter transformation:\")\n",
    "    print('Train: Max {}, min {}, mean {}, and var {}'.format(np.max(X_train_norm[:, i]), np.min(X_train_norm[:, i]), np.mean(X_train_norm[:, i]), np.var(X_train_norm[:, i])))\n",
    "    print('Test: Max {}, min {}, mean {}, and var {}'.format(np.max(X_test_norm[:, i]), np.min(X_test_norm[:, i]), np.mean(X_test_norm[:, i]), np.var(X_test_norm[:, i])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora comparemos los coeficientes que obtendr铆amos si us谩ramos la matriz de variables estandarizadas en su lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "lr_norm = linear_model.LinearRegression()\n",
    "ridge_norm = linear_model.Ridge(alpha=.3)\n",
    "lasso_norm = linear_model.Lasso(alpha=.3)\n",
    "\n",
    "lr_norm.fit(X_train_norm, y_train)\n",
    "ridge_norm.fit(X_train_norm, y_train)\n",
    "lasso_norm.fit(X_train_norm, y_train)\n",
    "\n",
    "coefs_lr_norm = pd.Series(np.abs(lr_norm.coef_), boston.feature_names).sort_values()\n",
    "coefs_ridge_norm = pd.Series(np.abs(ridge_norm.coef_), boston.feature_names).sort_values()\n",
    "coefs_lasso_norm = pd.Series(np.abs(lasso_norm.coef_), boston.feature_names).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15,5))\n",
    "ax1 = f.add_subplot(131)\n",
    "ax2 = f.add_subplot(132)\n",
    "ax3 = f.add_subplot(133)\n",
    "\n",
    "coefs_lr.plot(kind=\"barh\", title='coefs_lr', ax=ax1)\n",
    "coefs_ridge.plot(kind=\"barh\", title='coefs_ridge', ax=ax2)\n",
    "coefs_lasso.plot(kind=\"barh\", title='coefs_lasso', ax=ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15,5))\n",
    "ax1 = f.add_subplot(131)\n",
    "ax2 = f.add_subplot(132)\n",
    "ax3 = f.add_subplot(133)\n",
    "\n",
    "coefs_lr_norm.plot(kind=\"barh\", title='coefs_lr_norm', ax=ax1)\n",
    "coefs_ridge_norm.plot(kind=\"barh\", title='coefs_ridge_norm', ax=ax2)\n",
    "coefs_lasso_norm.plot(kind=\"barh\", title='coefs_lasso_norm', ax=ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Non important variables:')\n",
    "print('Before transformation: {}'.format(sorted(coefs_lasso.index[coefs_lasso_norm==0].values)))\n",
    "print('After transformation: {}'.format(sorted(coefs_lasso_norm.index[coefs_lasso_norm==0].values)))\n",
    "print('Most important variable:')\n",
    "print('Before transformation: {}'.format(coefs_lasso.index[-1]))\n",
    "print('After transformation: {}'.format(coefs_lasso_norm.index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "print('lr: {}'.format(lr.score(X_test, y_test)))\n",
    "print('ridge: {}'.format(ridge.score(X_test, y_test)))\n",
    "print('lasso: {}'.format(lasso.score(X_test, y_test)))\n",
    "print('lr_norm: {}'.format(lr_norm.score(X_test_norm, y_test)))\n",
    "print('ridge_norm: {}'.format(ridge_norm.score(X_test_norm, y_test)))\n",
    "print('lasso_norm: {}'.format(lasso_norm.score(X_test_norm, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustando los regresores Ridge y Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 100\n",
    "alphas = np.logspace(-2, 2, n_alphas)\n",
    "\n",
    "coefs_ridge = []\n",
    "r2_ridge = []\n",
    "for l in alphas:\n",
    "    regr_ridge = linear_model.Ridge(alpha=l) # Create a Ridge regressor\n",
    "    regr_ridge.fit(X_train_norm, y_train)  # Perform the fitting\n",
    "    coefs_ridge.append(regr_ridge.coef_)\n",
    "    r2_ridge.append(regr_ridge.score(X_test_norm,y_test))\n",
    "\n",
    "    \n",
    "coefs_lasso = []\n",
    "r2_lasso = []\n",
    "for l in alphas:\n",
    "    regr_lasso = linear_model.Lasso(alpha=l,tol =0.001) # Create a Ridge regressor\n",
    "    regr_lasso.fit(X_train_norm, y_train)  # Perform the fitting\n",
    "    coefs_lasso.append(regr_lasso.coef_)\n",
    "    r2_lasso.append(regr_lasso.score(X_test_norm,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 20), sharey='row')\n",
    "\n",
    "\n",
    "axs[0,0].plot(alphas, np.abs(coefs_ridge))\n",
    "axs[0,0].set_xscale('log')\n",
    "axs[0,0].set_title('Ridge coefficients as a function of the regularization')\n",
    "axs[0,0].axis('tight')\n",
    "axs[0,0].set_xlabel('alpha')\n",
    "axs[0,0].set_ylabel('weights')\n",
    "axs[0,0].legend(boston.feature_names)\n",
    "\n",
    "axs[0,1].plot(alphas, np.abs(coefs_lasso))\n",
    "axs[0,1].set_xscale('log')\n",
    "axs[0,1].set_title('Lasso coefficients as a function of the regularization')\n",
    "axs[0,1].axis('tight')\n",
    "axs[0,1].set_xlabel('alpha')\n",
    "axs[0,1].set_ylabel('weights')\n",
    "axs[0,1].legend(boston.feature_names)\n",
    "\n",
    "axs[1,0].plot(alphas, r2_ridge)\n",
    "axs[1,0].set_xscale('log')\n",
    "axs[1,0].set_title('Ridge scores as a function of the regularization')\n",
    "axs[1,0].axis('tight')\n",
    "axs[1,0].set_xlabel('alpha')\n",
    "axs[1,0].set_ylabel('r2')\n",
    "\n",
    "axs[1,1].plot(alphas, r2_lasso)\n",
    "axs[1,1].set_xscale('log')\n",
    "axs[1,1].set_title('Lasso scores as a function of the regularization')\n",
    "axs[1,1].axis('tight')\n",
    "axs[1,1].set_xlabel('alpha')\n",
    "axs[1,1].set_ylabel('r2')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal alphas\n",
    "best_r2_ridge = max(r2_ridge)\n",
    "max_index_ridge = r2_ridge.index(best_r2_ridge)\n",
    "best_alpha_ridge = alphas[max_index_ridge]\n",
    "print(max_index_ridge, best_alpha_ridge, best_r2_ridge, r2_ridge[max_index_ridge])\n",
    "\n",
    "best_r2_lasso = max(r2_lasso)\n",
    "max_index_lasso = r2_lasso.index(best_r2_lasso)\n",
    "best_alpha_lasso = alphas[max_index_lasso]\n",
    "print(max_index_lasso, best_alpha_lasso, best_r2_lasso, r2_lasso[max_index_lasso])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2score_test, r2score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(alpha=best_alpha_lasso)\n",
    "lasso.fit(X_train_norm, y_train)\n",
    "coefs = pd.Series(np.abs(lasso.coef_), features).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train_norm, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['targ'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['RM', 'NOX', 'DIS', 'PTRATIO', 'targ']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecci贸n de caracter铆sticas con Sklearn\n",
    "\n",
    "Tambi茅n podemos seleccionar las caracter铆sticas m谩s importantes con sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_selection as fs \n",
    "selector = fs.SelectKBest(score_func = fs.f_regression, k=5)\n",
    "\n",
    "X_new_train = selector.fit_transform(X_train,y_train)\n",
    "X_new_test = selector.transform(X_test)\n",
    "\n",
    "print('Non important variables: {}'.format(boston.feature_names[selector.get_support()==False]))\n",
    "print('Relevant variables: {}'.format(boston.feature_names[selector.get_support()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de caracter铆sticas seleccionadas ahora es diferente, ya que el criterio ha cambiado.\n",
    "\n",
    "El m茅todo SelectKBest selecciona caracter铆sticas seg煤n las k puntuaciones m谩s altas. La puntuaci贸n se calcula usando la funci贸n score_func.\n",
    "En este caso, utilizamos f_regression como nuestra funci贸n de puntuaci贸n, que devuelve la estad铆stica F y los valores p de las pruebas de regresi贸n lineal univariante de cada caracter铆stica en X contra y.\n",
    "\n",
    "**EJERCICIO 3** Diabetes:\n",
    "\n",
    "El conjunto de datos de diabetes (de scikit-learn) consta de 10 variables fisiol贸gicas (edad, sexo, peso, presi贸n arterial) medidas en 442 pacientes, y una indicaci贸n de la progresi贸n de la enfermedad despu茅s de un a帽o.\n",
    "\n",
    "Exploraremos el rendimiento del modelo de Regresi贸n Lineal y el modelo LASSO para la predicci贸n.\n",
    "\n",
    "Completa los huecos del ejercicio.\n",
    "\n",
    "Carga los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "X,y = diabetes.data, diabetes.target\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = diabetes.feature_names\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval煤a la predicci贸n usando un modelo de regresi贸n simple y un modelo de regresi贸n m煤ltiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns=features)\n",
    "df['targ'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = np.abs(df.corr())\n",
    "sns.heatmap(corr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df[['bmi', 's5', 'bp', 's4', 'targ']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo simple, primero elige una de las dimensiones de los datos. Intenta realizar algunos gr谩ficos para identificar posibles relaciones lineales entre las variables predictoras y las variables objetivo. Elige una variable para tu primer modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[['s5']]\n",
    "y_train = df_train['targ']\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[['s5']]\n",
    "y_test = df_test['targ']\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "print(lm.score(X_train, y_train), lm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide en conjuntos de entrenamiento y prueba y eval煤a la predicci贸n (sklearn) con un modelo de regresi贸n m煤ltiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns='targ')\n",
    "y_train = df_train['targ']\n",
    "\n",
    "X_test = df_test.drop(columns='targ')\n",
    "y_test = df_test['targ']\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "print(lm.score(X_train, y_train), lm.score(X_test, y_test))\n",
    "\n",
    "print(lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el modelo de regresi贸n m煤ltiple, divide en conjuntos de entrenamiento y prueba y eval煤a la predicci贸n (sklearn) sin y con regularizaci贸n LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_new = scaler.fit_transform(X_train)\n",
    "X_test_new = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.Lasso(alpha=1)\n",
    "lm.fit(X_train_new, y_train)\n",
    "print(lm.score(X_train_new, y_train), lm.score(X_test_new, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.abs(lm.coef_), features).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Almost the same results with less \"activated\" coefficients (the result has 3 zero coefficients).\n",
    " \n",
    " Is the score different? How many predictors are we using now?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EJERCICIO 4: Ventas de Big Mart**\n",
    "\n",
    "Usa el [conjunto de datos de ventas de Big Mart](https://www.kaggle.com/brijbhushannanda1979/bigmart-sales-data). En el conjunto de datos, tenemos ventas de productos por producto para m煤ltiples sucursales de una cadena.\n",
    "\n",
    "En particular, podemos ver caracter铆sticas del art铆culo vendido (contenido de grasa, visibilidad, tipo, precio) y algunas caracter铆sticas de la sucursal (a帽o de establecimiento, tama帽o, ubicaci贸n, tipo) y el n煤mero de art铆culos vendidos para ese art铆culo en particular. Veamos si podemos predecir las ventas usando estas caracter铆sticas.\n",
    "\n",
    "Implementa el siguiente an谩lisis:\n",
    "- Lee los archivos de entrenamiento y prueba en un DataFrame de pandas\n",
    "- Limpia los datos (hay algunos valores faltantes)\n",
    "- Convierte las variables categ贸ricas en valores num茅ricos y excluye 'Item_Identifier' y 'Item_Outlet_Sales' (que es el objetivo).\n",
    "- Estudia cu谩les son las variables con mayor (menor) correlaci贸n con la variable objetivo.\n",
    "- Aplica regresi贸n lineal usando todas las caracter铆sticas.\n",
    "- Construye el gr谩fico de residuos y da una interpretaci贸n del mismo\n",
    "- Elige un modelo de regresi贸n polin贸mica para ajustar mejor los datos.\n",
    "- Compara los regresores de ridge y lasso.\n",
    "- Compara la magnitud de los coeficientes de los diferentes modelos.\n",
    "- Estima cu谩les son las mejores caracter铆sticas para la predicci贸n.\n",
    "\n",
    "Lee los archivos de entrenamiento y prueba en un DataFrame de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "df_train = pd.read_csv('files/ch06/bigmart-sales-data/Train.csv')\n",
    "\n",
    "df_test = pd.read_csv('files/ch06/bigmart-sales-data/test.csv')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpia los datos (hay algunos valores faltantes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa los datos faltantes en Item_Weight y Outlet_Size. Veamos c贸mo se ven estas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.Item_Weight.mean(), df_train.Item_Weight.std())\n",
    "plt.hist(df_train.Item_Weight, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace nulls Item_Weight in with mean\n",
    "mean_Item_Weight = df_train.Item_Weight.mean()\n",
    "df_train2 =  df_train.copy()\n",
    "df_test2 =  df_test.copy()\n",
    "\n",
    "print(mean_Item_Weight)\n",
    "df_train2[['Item_Weight']] = df_train2[['Item_Weight']].fillna(value=mean_Item_Weight)\n",
    "df_test2[['Item_Weight']] = df_test2[['Item_Weight']].fillna(value=mean_Item_Weight)\n",
    "df_train2[df_train.Item_Weight.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Outlet_Size\", data=df_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[['Outlet_Size']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We'll fill the empty values of Outlet_Size with medium, since it麓s the most common value.\n",
    "df_train2[['Outlet_Size']] = df_train2[['Outlet_Size']].fillna(value='Medium')\n",
    "df_test2[['Outlet_Size']] = df_test2[['Outlet_Size']].fillna(value='Medium')\n",
    "df_train2[['Outlet_Size']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierte las variables categ贸ricas en valores num茅ricos y excluye 'Item_Identifier' y 'Item_Outlet_Sales' (que es el objetivo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train2['Item_Outlet_Sales']\n",
    "\n",
    "df_train2.drop(columns=['Item_Identifier','Item_Outlet_Sales'], inplace=True)\n",
    "df_test2.drop(columns=['Item_Identifier'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num = ['Item_Weight', 'Item_Visibility', 'Item_MRP','Outlet_Establishment_Year']\n",
    "cols_cat = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_cat:\n",
    "    print()\n",
    "    print(df_train2[[col]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = pd.get_dummies(df_train2, drop_first=True)\n",
    "df_test2 = pd.get_dummies(df_test2, drop_first=True)\n",
    "df_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudia cu谩les son las variables con la mayor (menor) correlaci贸n con la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']\n",
    "X_num = df_train2[cols_num].values\n",
    "X_num_test = df_test2[cols_num].values\n",
    "(X_num.shape, X_num_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalerX = StandardScaler().fit(X_num) # Create the transformer StandardScaler and perform the fitting for the training data\n",
    "\n",
    "X_num = scalerX.transform(X_num)\n",
    "X_num_test = scalerX.transform(X_num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2[cols_num] = X_num\n",
    "df_test2[cols_num] = X_num_test\n",
    "df_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2['y'] = y\n",
    "corr_mat = np.abs(df_train2.corr())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(corr_mat, square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat.y.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica regresi贸n lineal utilizando todas las caracter铆sticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train2.drop(columns=['y'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "y_pred = lr.predict(X)\n",
    "\n",
    "print(lr.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construye el gr谩fico de residuos y proporciona una interpretaci贸n del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = y_pred, y = y - y_pred, alpha=0.4)\n",
    "\n",
    "plt.hlines(y=0, xmin= 0, xmax=y_pred.max())\n",
    "plt.title('Residual plot')\n",
    "plt.xlabel('$\\hat y$')\n",
    "plt.ylabel('$y - \\hat y$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y, y_pred, alpha=0.3)\n",
    "\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], '--k')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('$y$')\n",
    "plt.ylabel('$\\hat y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elige un modelo de regresi贸n polin贸mica para ajustar mejor los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X2 = poly.fit_transform(X)\n",
    "\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit(X2, y)\n",
    "\n",
    "print(clf.score(X2, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compara los regresores de ridge y lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "#lr = LinearRegression()\n",
    "#lr.fit(X, y)\n",
    "\n",
    "ridge = Ridge(alpha=.5)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "lasso = Lasso(alpha=.5)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "print(lr.score(X, y), ridge.score(X, y), lasso.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compara la magnitud de los coeficientes de los diferentes modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_lr = pd.Series(lr.coef_, df_train2.columns[:-1]).sort_values()\n",
    "coefs_ridge = pd.Series(ridge.coef_, df_train2.columns[:-1]).sort_values()\n",
    "coefs_lasso = pd.Series(lasso.coef_, df_train2.columns[:-1]).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coefs_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coefs_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coefs_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estima cu谩les son las mejores caracter铆sticas para la predicci贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_train2.columns[:-1]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_selection as fs \n",
    "selector = fs.SelectKBest(score_func = fs.f_regression,k=5)\n",
    "\n",
    "X_new = selector.fit_transform(X,y)\n",
    "\n",
    "print('Non important variables: {}'.format(features[selector.get_support()==False]))\n",
    "print('Relevant variables: {}'.format(features[selector.get_support()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANLISIS ADICIONAL PARA LOS DATOS DE BOSTON**\n",
    "\n",
    "Para comparar el ajuste de los modelos de regresi贸n lineal y polin贸mica tambi茅n podemos usar la biblioteca sklearn.\n",
    "\n",
    "A continuaci贸n, a帽adimos una evaluaci贸n cuantitativa de los dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "X_boston,y_boston = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the linear model\n",
    "X_boston,y_boston = boston.data, boston.target\n",
    "\n",
    "regr_boston = LinearRegression()\n",
    "regr_boston.fit(X_boston, y_boston) \n",
    "\n",
    "#print('Coeff and intercept: {} {}'.format(regr_boston.coef_, regr_boston.intercept_))\n",
    "print('Multiple Linear regression Score: {}'.format(regr_boston.score(X_boston, y_boston)))\n",
    "print('Multiple Linear regression MSE: {}'.format(np.mean((regr_boston.predict(X_boston) - y_boston)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the polynomial model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "regr_pol = Pipeline([('poly', PolynomialFeatures(degree=2)),('linear', LinearRegression(fit_intercept=False))])\n",
    "regr_pol.fit(X_boston, y_boston) \n",
    "\n",
    "#print('Coeff and intercept: {} {}'.format(regr_pol.named_steps['linear'].coef_, regr_pol.named_steps['linear'].intercept_))\n",
    "print('Multiple Polynomial regression Score: {}'.format(regr_pol.score(X_boston, y_boston)))\n",
    "print('Multiple Polynomial regression MSE: {}'.format(np.mean((regr_pol.predict(X_boston) - y_boston)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la regresi贸n simple, primero necesitamos extraer una de las caracter铆sticas y luego usar los mismos m茅todos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative evaluation of the SIMPLE lineal and polynomial regression:\n",
    "bostonDF = pd.DataFrame(boston.data)\n",
    "bostonDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonDF.columns=boston.feature_names \n",
    "bostonDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=bostonDF['LSTAT']\n",
    "y=boston.target\n",
    "x = np.expand_dims(x, axis=1)\n",
    "y = np.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_boston = LinearRegression()\n",
    "regr_boston.fit(x, y) \n",
    "\n",
    "print('Simple linear regression Score: {}'.format(regr_boston.score(x, y)))\n",
    "print('Simple linear regression MSE: {}'.format(np.mean((regr_boston.predict(x) - y)**2)))\n",
    "\n",
    "regr_pol = Pipeline([('poly', PolynomialFeatures(degree=2)),('linear', LinearRegression(fit_intercept=False))])\n",
    "regr_pol.fit(x, y) \n",
    "\n",
    "print('Simple Polynomial regression (order 2) Score: {}'.format(regr_pol.score(x, y)))\n",
    "print('Simple Polynomial regression (order 2) MSE: {}'.format(np.mean((regr_pol.predict(x) - y)**2)))\n",
    "\n",
    "regr_pol = Pipeline([('poly', PolynomialFeatures(degree=3)),('linear', LinearRegression(fit_intercept=False))])\n",
    "regr_pol.fit(x, y) \n",
    "\n",
    "print('Simple Polynomial regression (order 3) Score: {}'.format(regr_pol.score(x, y)))\n",
    "print('Simple Polynomial regression (order 3) MSE: {}'.format(np.mean((regr_pol.predict(x) - y)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EJERCICIO 2: Conjunto de datos macroecon贸micos**\n",
    "\n",
    "Para comenzar, cargamos el conjunto de datos Longley de datos macroecon贸micos de EE. UU. desde el sitio web de conjuntos de datos de R. Datos macroecon贸micos desde 1947 hasta 1962."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/datasets/longley.csv', index_col=0)\n",
    "df.head()\n",
    "\n",
    "# Clean column names\n",
    "df.columns = ['GNPdeflator', 'GNP', 'Unemployed', 'ArmedForces', 'Population','Year', 'Employed']\n",
    "features = ['GNPdeflator','Unemployed','ArmedForces','Population','Year','Employed']\n",
    "target = 'GNP'\n",
    "\n",
    "# Create X matrix and y vector from the dataset\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "print('Shape of data: {} {}'.format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a multiple linear model\n",
    "lin_reg = LinearRegression() # Create the Linear Regression estimator\n",
    "lin_reg.fit(X, y) # Perform the fitting\n",
    "\n",
    "\n",
    "# Regrerssion coefs\n",
    "coefs = pd.Series(lin_reg.coef_, features).sort_values()\n",
    "\n",
    "# Prediction\n",
    "y_pred = lin_reg.predict(X)\n",
    "\n",
    "# evaluation\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2score = lin_reg.score(X, y)\n",
    "\n",
    "# The coefficients\n",
    "print('\\nIntercept and coefs:\\n{} {}'.format(lin_reg.intercept_, lin_reg.coef_))\n",
    "# The mean squared error\n",
    "print('\\nMSE: {}'.format(mse))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('R^2 Score: {}'.format(r2score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting abs value of model coefficients\n",
    "np.abs(coefs).sort_values().plot(kind='bar', title='Model Coefficients')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
